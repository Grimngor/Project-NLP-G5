{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Notbooks\n",
    "4. Natural language Processing IV (Section 5.4 thne 5.6)\n",
    "4. NLP Exercise - do with student\n",
    "\n",
    "### Steps to Follow\n",
    "1. Try NaiveBayes classiffier first\n",
    "2. Data Cleaning, lemming, etc\n",
    "3. TFIDF\n",
    "5. Try Random Forest (or K-means with only 2 clases?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Training Data file\n",
    "df= pd.read_csv('training_data_lowercase.csv', sep='\\t', names=['tag', 'text'])\n",
    "\n",
    "#Defining target and features\n",
    "X = df['text']\n",
    "y = df['tag']\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize resources and define Functions\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to get the correct WordNet POS tag\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,  # Adjectives\n",
    "                \"N\": wordnet.NOUN,  # Nouns\n",
    "                \"V\": wordnet.VERB,  # Verbs\n",
    "                \"R\": wordnet.ADV}   # Adverbs\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Default to noun if POS not found\n",
    "\n",
    "# Function to lemmatize tokens with POS tags\n",
    "def lemmatize_text(tokens):\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
    "\n",
    "# Function for flexible preprocessing\n",
    "def preprocess_text(text, clean=True, tokenize=True, lemmatize=True, remove_stopwords=True):\n",
    "    # Step 1: Basic Cleaning (lowercasing and removing non-alphabetic characters)\n",
    "    if clean:\n",
    "        text = text.lower()\n",
    "        text = ''.join([char if char.isalpha() or char.isspace() else ' ' for char in text])  # Keep spaces between words\n",
    "    \n",
    "    # Step 2: Tokenization\n",
    "    if tokenize:\n",
    "        tokens = word_tokenize(text)\n",
    "    else:\n",
    "        tokens = text.split()  # Split into words even if not tokenizing with NLTK\n",
    "    \n",
    "    # Step 3: Stopword Removal\n",
    "    if remove_stopwords:\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Step 4: Lemmatization with POS tagging\n",
    "    if lemmatize:\n",
    "        tokens = lemmatize_text(tokens)\n",
    "    \n",
    "    # Return processed text\n",
    "    return tokens if tokenize else ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training and test sets\n",
    "X_train_clean = X_train.apply(lambda x: preprocess_text(x, clean=True, tokenize=True, lemmatize=True, remove_stopwords=False))\n",
    "X_test_clean = X_test.apply(lambda x: preprocess_text(x, clean=True, tokenize=True, lemmatize=True, remove_stopwords=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tokens back into strings for vectorization\n",
    "X_train_final = X_train_clean.apply(lambda tokens: \" \".join(tokens))\n",
    "X_test_final = X_test_clean.apply(lambda tokens: \" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VECTORIZATION\n",
    "# Vectorization using bag of words\n",
    "bow_vect = CountVectorizer(max_features=1000)\n",
    "\n",
    "# Fit and transform the training data, transform the test data\n",
    "X_train_vect = bow_vect.fit_transform(X_train_final).toarray()\n",
    "X_test_vect = bow_vect.transform(X_test_final).toarray()\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vect = TfidfVectorizer(max_features=1000, ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform the training data, transform the test data\n",
    "X_train_tfidf = tfidf_vect.fit_transform(X_train_final).toarray()\n",
    "X_test_tfidf = tfidf_vect.transform(X_test_final).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING AND PREDICTING WITH DIFFERENT MODELS\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000).fit(X_train_vect, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test_vect)\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "svm_model = SVC(kernel='linear', random_state=42).fit(X_train_vect, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_vect)\n",
    "\n",
    "# Naive Bayes classifier\n",
    "nb_model = MultinomialNB().fit(X_train_vect, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test_vect)\n",
    "\n",
    "# XGB Classifier\n",
    "xgb_model = XGBClassifier(random_state=42).fit(list(X_train_vect), y_train)\n",
    "y_pred_xgb = xgb_model.predict(list(X_test_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save models for future use\n",
    "\"\"\" import pickle\n",
    "with open('svm_model_w2v.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_model, f)\n",
    "\n",
    "with open('logreg_model_w2v.pkl', 'wb') as f:\n",
    "    pickle.dump(logreg_model, f)\n",
    "\n",
    "with open('rf_model_w2v.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "\n",
    "with open('xgb_model_w2v.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confusion matrices\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score Log Regression: 0.9287\n",
      "Accuracy Score SVM: 0.9281\n",
      "Accuracy Score Naive Bayes: 0.9089\n",
      "Accuracy Score XGB Classifier: 0.9122\n"
     ]
    }
   ],
   "source": [
    "acc_score_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Accuracy Score Log Regression: {acc_score_lr:.4f}\")\n",
    "report_lr = classification_report(y_test, y_pred_lr)\n",
    "#print(\"Classification Report Log Regression:\\n\", report_lr)\n",
    "\n",
    "acc_score_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"Accuracy Score SVM: {acc_score_svm:.4f}\")\n",
    "report_svm = classification_report(y_test, y_pred_svm)\n",
    "#print(\"Classification Report SVM:\\n\", report_svm)\n",
    "\n",
    "acc_score_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(f\"Accuracy Score Naive Bayes: {acc_score_rf:.4f}\")\n",
    "report_nb = classification_report(y_test, y_pred_nb)\n",
    "#print(\"Classification Report Random Forest:\\n\", report_rf)\n",
    "\n",
    "acc_score_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"Accuracy Score XGB Classifier: {acc_score_xgb:.4f}\")\n",
    "report_xgb = classification_report(y_test, y_pred_xgb)\n",
    "#print(\"Classification Report XGB Classifier:\\n\", report_xgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
